{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5d85aa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T15:51:38.981629Z",
     "start_time": "2023-01-18T15:51:37.233417Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\pylab\\backend_inline.py:164: DeprecationWarning: `configure_inline_support` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.configure_inline_support()`\n",
      "  configure_inline_support(ip, backend)\n",
      "C:\\Anaconda3\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import base64, io\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "import glob\n",
    "\n",
    "from contextlib import closing\n",
    "from io import StringIO\n",
    "from os import path\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gym import Env, spaces, utils\n",
    "import torch.multiprocessing\n",
    "from gym.error import DependencyNotInstalled\n",
    "from gym.envs.toy_text.utils import categorical_sample\n",
    "import highway_env\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d05357",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T15:51:38.997555Z",
     "start_time": "2023-01-18T15:51:38.982596Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "class FlattenObsWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, dtype=np.float32):\n",
    "        super(FlattenObsWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.state_len = old_space.shape[0] * old_space.shape[1]\n",
    "        self.observation_space = gym.spaces.Box(old_space.low[0][0].repeat(self.state_len,\n",
    "                 axis=0),old_space.high[0][0].repeat(self.state_len, axis=0),\n",
    "                 dtype=dtype)\n",
    "    def reset(self, seed=0):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low,\n",
    "        dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "    def observation(self, observation):\n",
    "        self.buffer = np.array([elem for row in observation for elem in row])\n",
    "        return self.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d4d85",
   "metadata": {},
   "source": [
    "Environment from https://github.com/eleurent/highway-env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42871466",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T15:51:39.013513Z",
     "start_time": "2023-01-18T15:51:38.998554Z"
    }
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = self.fc1(state)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fallen-sailing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T15:51:39.028474Z",
     "start_time": "2023-01-18T15:51:39.016504Z"
    }
   },
   "outputs": [],
   "source": [
    "class OneHotDQN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(OneHotDQN, self).__init__()\n",
    "\n",
    "        num_feats = num_inputs\n",
    "        self.dummy_param = nn.Parameter(torch.empty(0))  # to get the device name designated to the module\n",
    "\n",
    "        self.q_layer = nn.Sequential(\n",
    "            nn.Linear(num_feats, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        # to get the device assigned to the module at initalization\n",
    "        device = self.dummy_param.device\n",
    "        if type(state) != torch.Tensor:\n",
    "            state = torch.Tensor(state).to(device=device)\n",
    "\n",
    "        q_vals = self.q_layer(state)\n",
    "        return q_vals\n",
    "    \n",
    "\n",
    "class OneHotValueNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs):\n",
    "        super(OneHotValueNetwork, self).__init__()\n",
    "\n",
    "        num_feats = num_inputs\n",
    "        self.dummy_param = nn.Parameter(torch.empty(0))  # to get the device name designated to the module\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(num_feats, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        # to get the device assigned to the module at initalization\n",
    "        device = self.dummy_param.device\n",
    "        if type(state) != torch.Tensor:\n",
    "            state = torch.Tensor(state).to(device=device)\n",
    "\n",
    "        val = self.value(state)\n",
    "        return val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-justice",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1587706a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T15:51:39.059391Z",
     "start_time": "2023-01-18T15:51:39.030468Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, alpha=0.05, gamma=0.99, seed=0, update_every=4, batch_size=128):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.gamma= gamma\n",
    "        self.update_interval = update_every\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=alpha)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        self.transition_matrix = np.zeros((state_size, action_size, state_size))\n",
    "        self.transition_matrix = np.zeros((state_size, action_size, state_size))\n",
    "        # Occupancy measure\n",
    "        self.occupancy = OccupancyVector()\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.occupancy.add(state, next_state)\n",
    "        #update state transition matrix\n",
    "        self.transition_matrix[np.argmax(state)][action][np.argmax(next_state)] = 1\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.update_interval\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.gamma)\n",
    "\n",
    "    def get_regret(self, state, prev_state, action):\n",
    "        self.qnetwork_local.eval()\n",
    "        #get regret for current action given potential true states\n",
    "        #potential true states given previous state\n",
    "        true_states = np.argmax(self.transition_matrix[np.argmax(prev_state)], axis=1)\n",
    "        true_states = np.array([statehelper(s) for s in true_states])\n",
    "        actions = np.full((self.action_size,1), action)\n",
    "        #print(state, true_states)\n",
    "        assert state in true_states\n",
    "\n",
    "        #expected value of current s,a\n",
    "        s = torch.tensor(state).float().unsqueeze(0).to(device)\n",
    "        a = torch.tensor([action]).long().unsqueeze(0).to(device)\n",
    "        v = self.qnetwork_local(s).gather(1, a).cpu().detach().numpy()\n",
    "        v = max(v,0)\n",
    "\n",
    "        #expected value of potential observations\n",
    "        ts = torch.tensor(true_states).float().unsqueeze(0).to(device)[0]\n",
    "        ta = torch.tensor(actions).long().unsqueeze(0).to(device)[0]\n",
    "        v_alt = self.qnetwork_local(ts).gather(1, ta).cpu().detach().numpy()\n",
    "        v_alt = np.where(v_alt<0,0,v_alt)\n",
    "\n",
    "        #assert v in v_alt\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        #regret calculation V(s,a) - min_V(true_s,a)\n",
    "        regret = np.max(v_alt) - v\n",
    "        if not isinstance(regret, np.ndarray):\n",
    "            return regret\n",
    "        return regret[0][0]\n",
    "\n",
    "    def get_regret_lower(self, state, prev_state, action):\n",
    "        #regret for lower policy, but meta agent contains actual values\n",
    "        self.qnetwork_local.eval()\n",
    "        #get regret for current action given potential true states\n",
    "        #potential true states given previous state\n",
    "        true_states = np.array([perturb_state(np.argmax(prev_state), a) for a in range(env.action_space.n)])\n",
    "        true_states = np.array([statehelper(s) for s in true_states])\n",
    "        actions = np.array([[perturb_state(np.argmax(s), action)] for s in true_states])\n",
    "        assert state in true_states\n",
    "\n",
    "        #expected value of current s,a\n",
    "        s = torch.tensor(state).float().unsqueeze(0).to(device)\n",
    "        a = torch.tensor([perturb_state(np.argmax(state), action)]).long().unsqueeze(0).to(device)\n",
    "        v = self.qnetwork_local(s).gather(1, a).cpu().detach().numpy()\n",
    "        v = max(v,0)\n",
    "\n",
    "        #expected value of potential observations\n",
    "        ts = torch.tensor(true_states).float().unsqueeze(0).to(device)[0]\n",
    "        ta = torch.tensor(actions).long().unsqueeze(0).to(device)[0]\n",
    "        v_alt = self.qnetwork_local(ts).gather(1, ta).cpu().detach().numpy()\n",
    "        v_alt = np.where(v_alt<0,0,v_alt)\n",
    "\n",
    "        assert s in ts\n",
    "        #assert a in ta\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        #regret calculation V(s,a) - min_V(true_s,a)\n",
    "        regret = np.max(v_alt) - v\n",
    "        if not isinstance(regret, np.ndarray):\n",
    "            return regret\n",
    "        return regret[0][0]\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        if(torch.is_tensor(state)):\n",
    "            state = state.float().unsqueeze(0).to(device)\n",
    "        else:\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        # Obtain random minibatch of tuples from D\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        ## Compute and minimize the loss\n",
    "        ### Extract next maximum estimated value from target network\n",
    "        q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "\n",
    "        ### Calculate target value from bellman equation\n",
    "        q_targets = rewards + gamma * q_targets_next * (1 - dones)\n",
    "        ### Calculate expected value from local network\n",
    "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        ### Loss calculation (we used Mean squared error)\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "orange-lobby",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T15:51:39.091304Z",
     "start_time": "2023-01-18T15:51:39.060388Z"
    }
   },
   "outputs": [],
   "source": [
    "class BVF_Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, constraint,alpha=0.05, gamma=0.99, seed=0, update_every=4, batch_size=128):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.constraint = constraint\n",
    "        self.seed = random.seed(seed)\n",
    "        self.gamma= gamma\n",
    "        self.update_interval = update_every\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=alpha)\n",
    "        \n",
    "        # Q-Cost-Network\n",
    "        self.qnetwork_forward = OneHotDQN(state_size, action_size).to(device)\n",
    "        self.optimizer1 = optim.Adam(self.qnetwork_forward.parameters(), lr=alpha)\n",
    "        self.qnetwork_backward = OneHotValueNetwork(state_size).to(device)\n",
    "        self.optimizer2 = optim.Adam(self.qnetwork_backward.parameters(), lr=alpha)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = Safe_ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        self.transition_matrix = np.zeros((state_size, action_size, state_size))\n",
    "        self.transition_matrix = np.zeros((state_size, action_size, state_size))\n",
    "        # Occupancy measure\n",
    "        self.occupancy = OccupancyVector()\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, constraint, action, reward, next_state, next_constraint, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state,constraint, action, reward, next_state, next_constraint, done)\n",
    "        self.occupancy.add(state, next_state)\n",
    "        #update state transition matrix\n",
    "        self.transition_matrix[np.argmax(state)][action][np.argmax(next_state)] = 1\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.update_interval\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.gamma)\n",
    "\n",
    "    def get_regret(self, state, prev_state, action):\n",
    "        self.qnetwork_local.eval()\n",
    "        #get regret for current action given potential true states\n",
    "        #potential true states given previous state\n",
    "        true_states = np.argmax(self.transition_matrix[np.argmax(prev_state)], axis=1)\n",
    "        true_states = np.array([statehelper(s) for s in true_states])\n",
    "        actions = np.full((self.action_size,1), action)\n",
    "        #print(state, true_states)\n",
    "        assert state in true_states\n",
    "\n",
    "        #expected value of current s,a\n",
    "        s = torch.tensor(state).float().unsqueeze(0).to(device)\n",
    "        a = torch.tensor([action]).long().unsqueeze(0).to(device)\n",
    "        v = self.qnetwork_local(s).gather(1, a).cpu().detach().numpy()\n",
    "        v = max(v,0)\n",
    "\n",
    "        #expected value of potential observations\n",
    "        ts = torch.tensor(true_states).float().unsqueeze(0).to(device)[0]\n",
    "        ta = torch.tensor(actions).long().unsqueeze(0).to(device)[0]\n",
    "        v_alt = self.qnetwork_local(ts).gather(1, ta).cpu().detach().numpy()\n",
    "        v_alt = np.where(v_alt<0,0,v_alt)\n",
    "\n",
    "        #assert v in v_alt\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        #regret calculation V(s,a) - min_V(true_s,a)\n",
    "        regret = np.max(v_alt) - v\n",
    "        if not isinstance(regret, np.ndarray):\n",
    "            return regret\n",
    "        return regret[0][0]\n",
    "\n",
    "    def get_regret_lower(self, state, prev_state, action):\n",
    "        #regret for lower policy, but meta agent contains actual values\n",
    "        self.qnetwork_local.eval()\n",
    "        #get regret for current action given potential true states\n",
    "        #potential true states given previous state\n",
    "        true_states = np.array([perturb_state(np.argmax(prev_state), a) for a in range(env.action_space.n)])\n",
    "        true_states = np.array([statehelper(s) for s in true_states])\n",
    "        actions = np.array([[perturb_state(np.argmax(s), action)] for s in true_states])\n",
    "        assert state in true_states\n",
    "\n",
    "        #expected value of current s,a\n",
    "        s = torch.tensor(state).float().unsqueeze(0).to(device)\n",
    "        a = torch.tensor([perturb_state(np.argmax(state), action)]).long().unsqueeze(0).to(device)\n",
    "        v = self.qnetwork_local(s).gather(1, a).cpu().detach().numpy()\n",
    "        v = max(v,0)\n",
    "\n",
    "        #expected value of potential observations\n",
    "        ts = torch.tensor(true_states).float().unsqueeze(0).to(device)[0]\n",
    "        ta = torch.tensor(actions).long().unsqueeze(0).to(device)[0]\n",
    "        v_alt = self.qnetwork_local(ts).gather(1, ta).cpu().detach().numpy()\n",
    "        v_alt = np.where(v_alt<0,0,v_alt)\n",
    "\n",
    "        assert s in ts\n",
    "        #assert a in ta\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        #regret calculation V(s,a) - min_V(true_s,a)\n",
    "        regret = np.max(v_alt) - v\n",
    "        if not isinstance(regret, np.ndarray):\n",
    "            return regret\n",
    "        return regret[0][0]\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        cons = 0       \n",
    "        if state[3].item()>0.25 and 0.5<state[2].item()<1:\n",
    "                cons = 1  \n",
    "        if(torch.is_tensor(state)):\n",
    "            state = state.float().unsqueeze(0).to(device)\n",
    "        else:\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "            \n",
    "        c_f = self.qnetwork_forward(state)\n",
    "        c_b = self.qnetwork_backward(state)\n",
    "        quantity_1 = c_f + c_b\n",
    "        quantity_2 = self.constraint + cons\n",
    "        \n",
    "        constraint_mask = torch.le(quantity_1, quantity_2).float().squeeze(0)\n",
    "        filtered_Q = (action_values + 1000.0) * (constraint_mask)\n",
    "        filtered_action = np.argmax(filtered_Q.cpu().data.numpy())\n",
    "        \n",
    "        alt_Q = -1. * c_f\n",
    "        alt_action =np.argmax(alt_Q.cpu().data.numpy())\n",
    "        c_sum = constraint_mask.sum(0)\n",
    "        action_mask = (c_sum == torch.zeros_like(c_sum)).cpu().numpy()\n",
    "        action = (1 - action_mask) * filtered_action + action_mask * alt_action\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            #return np.argmax(action_values.cpu().data.numpy())\n",
    "            return action, c_f[0][action].item(), c_b[0].item()\n",
    "        else:\n",
    "            action_r = random.choice(np.arange(self.action_size))\n",
    "            return action_r, c_f[0][action_r].item(), c_b[0].item()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        # Obtain random minibatch of tuples from D\n",
    "        states, constraints, actions, rewards, next_states, next_constraints, dones = experiences\n",
    "       \n",
    "        ## Compute and minimize the loss\n",
    "        ### Extract next maximum estimated value from target network\n",
    "        q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "\n",
    "        ### Calculate target value from bellman equation\n",
    "        q_targets = rewards + gamma * q_targets_next * (1 - dones)\n",
    "        ### Calculate expected value from local network\n",
    "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        ### Loss calculation (we used Mean squared error)\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        \n",
    "        ### Calculate target cost from bellman equation\n",
    "        c_targets1 = constraints\n",
    "        #c_targets1 = c_targets1.gather(1, actions)\n",
    "        ### Calculate expected cost from qnetwork_forward\n",
    "        c_expected1 = self.qnetwork_forward(next_states).gather(1, actions)\n",
    "        ### Loss calculation (we used Mean squared error)\n",
    "        loss1 = F.mse_loss(c_expected1, c_targets1)\n",
    "        self.optimizer1.zero_grad()\n",
    "        loss1.backward()\n",
    "        self.optimizer1.step()\n",
    "        \n",
    "\n",
    "        ### Calculate target cost from bellman equation\n",
    "        c_targets2 = constraints\n",
    "        #c_targets2 = c_targets2.gather(1, actions)\n",
    "        ### Calculate expected cost from qnetwork_backward\n",
    "        c_expected2 = self.qnetwork_backward(next_states)\n",
    "        ### Loss calculation (we used Mean squared error)\n",
    "        loss2 = F.mse_loss(c_expected2, c_targets2)\n",
    "        self.optimizer2.zero_grad()\n",
    "        loss2.backward()\n",
    "        self.optimizer2.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hundred-anthropology",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T15:51:39.123251Z",
     "start_time": "2023-01-18T15:51:39.093308Z"
    }
   },
   "outputs": [],
   "source": [
    "class Safe_Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, constraint, alpha=0.05, gamma=0.99, seed=0, update_every=4, batch_size=128):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            constraint (int): Number of constraint\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.constraint = constraint\n",
    "        self.seed = random.seed(seed)\n",
    "        self.gamma= gamma\n",
    "        self.update_interval = update_every\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size+1, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size+1, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=alpha)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = Safe_ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        self.transition_matrix = np.zeros((state_size, action_size, state_size))\n",
    "        self.transition_matrix = np.zeros((state_size, action_size, state_size))\n",
    "        # Occupancy measure\n",
    "        self.occupancy = OccupancyVector()\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, constraint, action, reward, next_state, next_constraint, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state,constraint, action, reward, next_state, next_constraint, done)\n",
    "        self.occupancy.add(state, next_state)\n",
    "        #update state transition matrix\n",
    "        self.transition_matrix[np.argmax(state)][action][np.argmax(next_state)] = 1\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.update_interval\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.gamma)\n",
    "\n",
    "    def get_regret(self, state, prev_state, action):\n",
    "        self.qnetwork_local.eval()\n",
    "        #get regret for current action given potential true states\n",
    "        #potential true states given previous state\n",
    "        true_states = np.argmax(self.transition_matrix[np.argmax(prev_state)], axis=1)\n",
    "        true_states = np.array([statehelper(s) for s in true_states])\n",
    "        actions = np.full((self.action_size,1), action)\n",
    "        #print(state, true_states)\n",
    "        assert state in true_states\n",
    "\n",
    "        #expected value of current s,a\n",
    "        s = torch.tensor(state).float().unsqueeze(0).to(device)\n",
    "        a = torch.tensor([action]).long().unsqueeze(0).to(device)\n",
    "        v = self.qnetwork_local(s).gather(1, a).cpu().detach().numpy()\n",
    "        v = max(v,0)\n",
    "\n",
    "        #expected value of potential observations\n",
    "        ts = torch.tensor(true_states).float().unsqueeze(0).to(device)[0]\n",
    "        ta = torch.tensor(actions).long().unsqueeze(0).to(device)[0]\n",
    "        v_alt = self.qnetwork_local(ts).gather(1, ta).cpu().detach().numpy()\n",
    "        v_alt = np.where(v_alt<0,0,v_alt)\n",
    "\n",
    "        #assert v in v_alt\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        #regret calculation V(s,a) - min_V(true_s,a)\n",
    "        regret = np.max(v_alt) - v\n",
    "        if not isinstance(regret, np.ndarray):\n",
    "            return regret\n",
    "        return regret[0][0]\n",
    "\n",
    "    def get_regret_lower(self, state, prev_state, action):\n",
    "        #regret for lower policy, but meta agent contains actual values\n",
    "        self.qnetwork_local.eval()\n",
    "        #get regret for current action given potential true states\n",
    "        #potential true states given previous state\n",
    "        true_states = np.array([perturb_state(np.argmax(prev_state), a) for a in range(env.action_space.n)])\n",
    "        true_states = np.array([statehelper(s) for s in true_states])\n",
    "        actions = np.array([[perturb_state(np.argmax(s), action)] for s in true_states])\n",
    "        assert state in true_states\n",
    "\n",
    "        #expected value of current s,a\n",
    "        s = torch.tensor(state).float().unsqueeze(0).to(device)\n",
    "        a = torch.tensor([perturb_state(np.argmax(state), action)]).long().unsqueeze(0).to(device)\n",
    "        v = self.qnetwork_local(s).gather(1, a).cpu().detach().numpy()\n",
    "        v = max(v,0)\n",
    "\n",
    "        #expected value of potential observations\n",
    "        ts = torch.tensor(true_states).float().unsqueeze(0).to(device)[0]\n",
    "        ta = torch.tensor(actions).long().unsqueeze(0).to(device)[0]\n",
    "        v_alt = self.qnetwork_local(ts).gather(1, ta).cpu().detach().numpy()\n",
    "        v_alt = np.where(v_alt<0,0,v_alt)\n",
    "\n",
    "        assert s in ts\n",
    "        #assert a in ta\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        #regret calculation V(s,a) - min_V(true_s,a)\n",
    "        regret = np.max(v_alt) - v\n",
    "        if not isinstance(regret, np.ndarray):\n",
    "            return regret\n",
    "        return regret[0][0]\n",
    "\n",
    "    def act(self, state_constraint, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        if(torch.is_tensor(state_constraint)):\n",
    "            state_constraint = state_constraint.float().unsqueeze(0).to(device)\n",
    "        else:\n",
    "            state_constraint = torch.from_numpy(state_constraint).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state_constraint)\n",
    "        self.qnetwork_local.train()\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        # Obtain random minibatch of tuples from D\n",
    "        states, constraints, actions, rewards, next_states, next_constraints, dones = experiences\n",
    "        next_states_next_constraints=torch.cat((next_states,next_constraints),dim=1)\n",
    "        states_constraints=torch.cat((states,constraints),dim=1)\n",
    "        ## Compute and minimize the loss\n",
    "        ### Extract next maximum estimated value from target network\n",
    "        q_targets_next = self.qnetwork_target(next_states_next_constraints).detach().max(1)[0].unsqueeze(1)\n",
    "\n",
    "        ### Calculate target value from bellman equation\n",
    "        constraint_mask = torch.le(constraints, self.constraint).float().squeeze(0)\n",
    "        r = rewards\n",
    "        for i in range(len(constraint_mask)):\n",
    "            if constraint_mask[i] == 1:\n",
    "                r[i] = rewards[i]\n",
    "            else:\n",
    "                r[i] = -10\n",
    "        q_targets = r + gamma * q_targets_next * (1 - dones)\n",
    "        #q_targets = rewards + gamma * q_targets_next * (1 - dones)\n",
    "        ### Calculate expected value from local network\n",
    "        q_expected = self.qnetwork_local(states_constraints).gather(1, actions)\n",
    "        ### Loss calculation (we used Mean squared error)\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f242022",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T15:51:39.154863Z",
     "start_time": "2023-01-18T15:51:39.126212Z"
    }
   },
   "outputs": [],
   "source": [
    "class LYP_Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, constraint, alpha=0.05, gamma=0.99, seed=0, update_every=4, batch_size=128):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.gamma= gamma\n",
    "        self.update_interval = update_every\n",
    "        self.batch_size = batch_size\n",
    "        self.constraint = constraint\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(self.state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(self.state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=alpha)\n",
    "        \n",
    "        # Q-Cost-Network\n",
    "        self.qnetwork_forward = OneHotDQN(self.state_size, action_size).to(device)\n",
    "        self.qnetwork_forward_target = OneHotDQN(self.state_size, action_size).to(device)\n",
    "        self.optimizer_c = optim.Adam(self.qnetwork_forward.parameters(), lr=alpha)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        self.transition_matrix = np.zeros((self.state_size, action_size, self.state_size))\n",
    "        self.transition_matrix = np.zeros((self.state_size, action_size, self.state_size))\n",
    "        # Occupancy measure\n",
    "        self.occupancy = OccupancyVector()\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.occupancy.add(state, next_state)\n",
    "        #update state transition matrix\n",
    "        self.transition_matrix[np.argmax(state)][action][np.argmax(next_state)] = 1\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.update_interval\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.gamma)\n",
    "\n",
    "    def get_regret(self, state, prev_state, action):\n",
    "        self.qnetwork_local.eval()\n",
    "        #get regret for current action given potential true states\n",
    "        #potential true states given previous state\n",
    "        true_states = np.argmax(self.transition_matrix[np.argmax(prev_state)], axis=1)\n",
    "        true_states = np.array([statehelper(s) for s in true_states])\n",
    "        actions = np.full((self.action_size,1), action)\n",
    "        #print(state, true_states)\n",
    "        assert state in true_states\n",
    "\n",
    "        #expected value of current s,a\n",
    "        s = torch.tensor(state).float().unsqueeze(0).to(device)\n",
    "        a = torch.tensor([action]).long().unsqueeze(0).to(device)\n",
    "        v = self.qnetwork_local(s).gather(1, a).cpu().detach().numpy()\n",
    "        v = max(v,0)\n",
    "\n",
    "        #expected value of potential observations\n",
    "        ts = torch.tensor(true_states).float().unsqueeze(0).to(device)[0]\n",
    "        ta = torch.tensor(actions).long().unsqueeze(0).to(device)[0]\n",
    "        v_alt = self.qnetwork_local(ts).gather(1, ta).cpu().detach().numpy()\n",
    "        v_alt = np.where(v_alt<0,0,v_alt)\n",
    "\n",
    "        #assert v in v_alt\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        #regret calculation V(s,a) - min_V(true_s,a)\n",
    "        regret = np.max(v_alt) - v\n",
    "        if not isinstance(regret, np.ndarray):\n",
    "            return regret\n",
    "        return regret[0][0]\n",
    "\n",
    "    def get_regret_lower(self, state, prev_state, action):\n",
    "        #regret for lower policy, but meta agent contains actual values\n",
    "        self.qnetwork_local.eval()\n",
    "        #get regret for current action given potential true states\n",
    "        #potential true states given previous state\n",
    "        true_states = np.array([perturb_state(np.argmax(prev_state), a) for a in range(env.action_space.n)])\n",
    "        true_states = np.array([statehelper(s) for s in true_states])\n",
    "        actions = np.array([[perturb_state(np.argmax(s), action)] for s in true_states])\n",
    "        assert state in true_states\n",
    "\n",
    "        #expected value of current s,a\n",
    "        s = torch.tensor(state).float().unsqueeze(0).to(device)\n",
    "        a = torch.tensor([perturb_state(np.argmax(state), action)]).long().unsqueeze(0).to(device)\n",
    "        v = self.qnetwork_local(s).gather(1, a).cpu().detach().numpy()\n",
    "        v = max(v,0)\n",
    "\n",
    "        #expected value of potential observations\n",
    "        ts = torch.tensor(true_states).float().unsqueeze(0).to(device)[0]\n",
    "        ta = torch.tensor(actions).long().unsqueeze(0).to(device)[0]\n",
    "        v_alt = self.qnetwork_local(ts).gather(1, ta).cpu().detach().numpy()\n",
    "        v_alt = np.where(v_alt<0,0,v_alt)\n",
    "\n",
    "        assert s in ts\n",
    "        #assert a in ta\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        #regret calculation V(s,a) - min_V(true_s,a)\n",
    "        regret = np.max(v_alt) - v\n",
    "        if not isinstance(regret, np.ndarray):\n",
    "            return regret\n",
    "        return regret[0][0]\n",
    "    \n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        cons = 0\n",
    "        if state[3].item()>0.25 and 0.5<state[2].item()<1:\n",
    "                cons = 1 \n",
    "        if(torch.is_tensor(state)):\n",
    "            state = state.float().unsqueeze(0).to(device)\n",
    "        else:\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            q_value = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        self.qnetwork_forward.eval()\n",
    "        with torch.no_grad():\n",
    "            cost_q_val = self.qnetwork_forward(state)\n",
    "        self.qnetwork_forward.train()\n",
    "        \n",
    "        max_q_val = cost_q_val.max(1)[0].unsqueeze(1)\n",
    "        \n",
    "        epsilon = (1 - self.gamma) * (self.constraint - cons)\n",
    "        \n",
    "        constraint_mask = torch.le(cost_q_val , epsilon + max_q_val).float()\n",
    "\n",
    "        filtered_Q = (q_value + 1000.0) * (constraint_mask)\n",
    "\n",
    "        filtered_action = filtered_Q.max(1)[1].cpu().numpy()\n",
    "        \n",
    "        alt_action = (-1. * cost_q_val).max(1)[1].cpu().numpy()\n",
    "        \n",
    "        c_sum = constraint_mask.sum(1)\n",
    "        action_mask = (c_sum == torch.zeros_like(c_sum)).cpu().numpy()\n",
    "        \n",
    "        action = (1 - action_mask) * filtered_action + action_mask * alt_action\n",
    "        \n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            #return np.argmax(action_values.cpu().data.numpy())\n",
    "            return action[0]\n",
    "        else:\n",
    "            action_r = random.choice(np.arange(self.action_size))\n",
    "            return action_r\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        # Obtain random minibatch of tuples from D\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        ## Compute and minimize the loss\n",
    "        ### Extract next maximum estimated value from target network\n",
    "        q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "\n",
    "        ### Calculate target value from bellman equation\n",
    "        q_targets = rewards + gamma * q_targets_next * (1 - dones)\n",
    "        ### Calculate expected value from local network\n",
    "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        ### Loss calculation (we used Mean squared error)\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        ### Calculate target cost from bellman equation\n",
    "        c_targets_c = self.qnetwork_forward_target(next_states).detach().min(1)[0].unsqueeze(1)\n",
    "        \n",
    "        ### Calculate expected cost from qnetwork_forward\n",
    "        c_expected_c = self.qnetwork_forward(states).gather(1, actions)\n",
    "        ### Loss calculation (we used Mean squared error)\n",
    "        loss_c = F.mse_loss(c_expected_c, c_targets_c)\n",
    "        self.optimizer_c.zero_grad()\n",
    "        loss_c.backward()\n",
    "        self.optimizer_c.step()\n",
    "\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
    "        self.soft_update(self.qnetwork_forward, self.qnetwork_forward_target, TAU)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "enclosed-wellington",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T15:51:39.232621Z",
     "start_time": "2023-01-18T15:51:39.156823Z"
    }
   },
   "outputs": [],
   "source": [
    "class SAC_Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, constraint,alpha=0.05, gamma=0.99, seed=0, update_every=4, batch_size=128):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.constraint = constraint\n",
    "        self.seed = random.seed(seed)\n",
    "        self.gamma= gamma\n",
    "        self.update_interval = update_every\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Q-Networks\n",
    "        self.qnetwork_local_1 = QNetwork(state_size+1, action_size, seed).to(device)\n",
    "        self.qnetwork_target_1 = QNetwork(state_size+1, action_size, seed).to(device)\n",
    "        self.optimizer_1 = optim.Adam(self.qnetwork_local_1.parameters(), lr=alpha)\n",
    "        \n",
    "        self.qnetwork_local_2 = QNetwork(state_size+1, action_size, seed).to(device)\n",
    "        self.qnetwork_target_2 = QNetwork(state_size+1, action_size, seed).to(device)\n",
    "        self.optimizer_2 = optim.Adam(self.qnetwork_local_2.parameters(), lr=alpha)\n",
    "        \n",
    "        # Q-Cost-Network\n",
    "        self.qnetwork_forward_1 = OneHotDQN(state_size+1, action_size).to(device)\n",
    "        self.qnetwork_forward_target_1 = OneHotDQN(state_size+1, action_size).to(device)\n",
    "        self.optimizer_c_1 = optim.Adam(self.qnetwork_forward_1.parameters(), lr=alpha)\n",
    "        \n",
    "        self.qnetwork_forward_2 = OneHotDQN(state_size+1, action_size).to(device)\n",
    "        self.qnetwork_forward_target_2 = OneHotDQN(state_size+1, action_size).to(device)\n",
    "        self.optimizer_c_2 = optim.Adam(self.qnetwork_forward_2.parameters(), lr=alpha)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = Safe_ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        self.transition_matrix = np.zeros((state_size, action_size, state_size))\n",
    "        self.transition_matrix = np.zeros((state_size, action_size, state_size))\n",
    "        # Occupancy measure\n",
    "        self.occupancy = OccupancyVector()\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, constraint, action, reward, next_state, next_constraint, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state,constraint, action, reward, next_state, next_constraint, done)\n",
    "        self.occupancy.add(state, next_state)\n",
    "        #update state transition matrix\n",
    "        self.transition_matrix[np.argmax(state)][action][np.argmax(next_state)] = 1\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.update_interval\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.gamma)\n",
    "\n",
    "    def get_regret(self, state, prev_state, action):\n",
    "        self.qnetwork_local.eval()\n",
    "        #get regret for current action given potential true states\n",
    "        #potential true states given previous state\n",
    "        true_states = np.argmax(self.transition_matrix[np.argmax(prev_state)], axis=1)\n",
    "        true_states = np.array([statehelper(s) for s in true_states])\n",
    "        actions = np.full((self.action_size,1), action)\n",
    "        #print(state, true_states)\n",
    "        assert state in true_states\n",
    "\n",
    "        #expected value of current s,a\n",
    "        s = torch.tensor(state).float().unsqueeze(0).to(device)\n",
    "        a = torch.tensor([action]).long().unsqueeze(0).to(device)\n",
    "        v = self.qnetwork_local(s).gather(1, a).cpu().detach().numpy()\n",
    "        v = max(v,0)\n",
    "\n",
    "        #expected value of potential observations\n",
    "        ts = torch.tensor(true_states).float().unsqueeze(0).to(device)[0]\n",
    "        ta = torch.tensor(actions).long().unsqueeze(0).to(device)[0]\n",
    "        v_alt = self.qnetwork_local(ts).gather(1, ta).cpu().detach().numpy()\n",
    "        v_alt = np.where(v_alt<0,0,v_alt)\n",
    "\n",
    "        #assert v in v_alt\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        #regret calculation V(s,a) - min_V(true_s,a)\n",
    "        regret = np.max(v_alt) - v\n",
    "        if not isinstance(regret, np.ndarray):\n",
    "            return regret\n",
    "        return regret[0][0]\n",
    "\n",
    "    def get_regret_lower(self, state, prev_state, action):\n",
    "        #regret for lower policy, but meta agent contains actual values\n",
    "        self.qnetwork_local.eval()\n",
    "        #get regret for current action given potential true states\n",
    "        #potential true states given previous state\n",
    "        true_states = np.array([perturb_state(np.argmax(prev_state), a) for a in range(env.action_space.n)])\n",
    "        true_states = np.array([statehelper(s) for s in true_states])\n",
    "        actions = np.array([[perturb_state(np.argmax(s), action)] for s in true_states])\n",
    "        assert state in true_states\n",
    "\n",
    "        #expected value of current s,a\n",
    "        s = torch.tensor(state).float().unsqueeze(0).to(device)\n",
    "        a = torch.tensor([perturb_state(np.argmax(state), action)]).long().unsqueeze(0).to(device)\n",
    "        v = self.qnetwork_local(s).gather(1, a).cpu().detach().numpy()\n",
    "        v = max(v,0)\n",
    "\n",
    "        #expected value of potential observations\n",
    "        ts = torch.tensor(true_states).float().unsqueeze(0).to(device)[0]\n",
    "        ta = torch.tensor(actions).long().unsqueeze(0).to(device)[0]\n",
    "        v_alt = self.qnetwork_local(ts).gather(1, ta).cpu().detach().numpy()\n",
    "        v_alt = np.where(v_alt<0,0,v_alt)\n",
    "\n",
    "        assert s in ts\n",
    "        #assert a in ta\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        #regret calculation V(s,a) - min_V(true_s,a)\n",
    "        regret = np.max(v_alt) - v\n",
    "        if not isinstance(regret, np.ndarray):\n",
    "            return regret\n",
    "        return regret[0][0]\n",
    "\n",
    "    def act(self, state, constraint, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        cons = 0\n",
    "        lamb = 1\n",
    "        if state[3].item()>0.25 and 0.5<state[2].item()<1:\n",
    "                cons = 1 \n",
    "        state_constraint = torch.cat((state, constraint))\n",
    "        if(torch.is_tensor(state_constraint)):\n",
    "            state_constraint = state_constraint.float().unsqueeze(0).to(device)\n",
    "        else:\n",
    "            state_constraint = torch.from_numpy(state_constraint).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local_1.eval()\n",
    "        self.qnetwork_local_2.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = torch.zeros(1,self.action_size).to(device)\n",
    "            for i in range(self.action_size):\n",
    "                action_values[0][i] = min(self.qnetwork_local_1(state_constraint)[0][i],self.qnetwork_local_2(state_constraint)[0][i])\n",
    "        self.qnetwork_local_1.train()\n",
    "        self.qnetwork_local_2.train()\n",
    "        #print(\"Values: \",action_values)\n",
    "        with torch.no_grad():\n",
    "            c_f_1 = self.qnetwork_forward_1(state_constraint)\n",
    "            c_f_2 = self.qnetwork_forward_2(state_constraint)\n",
    "            c_f = c_f_1\n",
    "            for i in range(len(c_f)):\n",
    "                c_f[0][i] = max(c_f_1[0][i],c_f_2[0][i])\n",
    "        #print(c_f_1)\n",
    "        #print(c_f_2)\n",
    "        #print(c_f)\n",
    "        #print()\n",
    "        quantity_1 = c_f + constraint[0].item()\n",
    "        quantity_2 = self.constraint + cons\n",
    "        \n",
    "        constraint_mask = torch.le(quantity_1, quantity_2).float().squeeze(0)\n",
    "        filtered_Q = (action_values + 1000.0) * (constraint_mask)\n",
    "        filtered_action = np.argmax(filtered_Q.cpu().data.numpy())\n",
    "        \n",
    "        alt_Q = - lamb * c_f\n",
    "        alt_action =np.argmax(alt_Q.cpu().data.numpy())\n",
    "        c_sum = constraint_mask.sum(0)\n",
    "        action_mask = (c_sum == torch.zeros_like(c_sum)).cpu().numpy()\n",
    "        action = (1 - action_mask) * filtered_action + action_mask * alt_action\n",
    "        \n",
    "        a=0\n",
    "        if c_sum == torch.zeros_like(c_sum):\n",
    "            a = 1\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            #return np.argmax(action_values.cpu().data.numpy())\n",
    "            return action, a, c_f[0][action].item()\n",
    "        else:\n",
    "            action_r = random.choice(np.arange(self.action_size))\n",
    "            return action_r, a, c_f[0][action_r].item()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        alpha = 0.9\n",
    "        # Obtain random minibatch of tuples from D\n",
    "        states, constraints, actions, rewards, next_states, next_constraints, dones = experiences\n",
    "       \n",
    "        ## Compute and minimize the loss\n",
    "        ### Extract next maximum estimated value from target network\n",
    "        next_states_next_constraints=torch.cat((next_states,next_constraints),dim=1)\n",
    "        states_constraints=torch.cat((states,constraints),dim=1)\n",
    "        \n",
    "        ## Compute and minimize the loss\n",
    "        ### Extract next maximum estimated value from target network\n",
    "        q_targets_next_1 = self.qnetwork_target_1(next_states_next_constraints).detach().max(1)[0].unsqueeze(1)\n",
    "        q_targets_next_2 = self.qnetwork_target_2(next_states_next_constraints).detach().max(1)[0].unsqueeze(1)\n",
    "        q_targets_next = q_targets_next_1\n",
    "        for i in range(len(q_targets_next_1)):\n",
    "            q_targets_next[i] = min(q_targets_next_1[i],q_targets_next_2[i]) - alpha*math.log(((actions == actions[i].item()).sum()/len(actions)).item())\n",
    "        ### Calculate target value from bellman equation\n",
    "        q_targets = rewards + gamma * q_targets_next * (1 - dones)\n",
    "        ### Calculate expected value from local network\n",
    "        q_expected_1 = self.qnetwork_local_1(states_constraints).gather(1, actions)\n",
    "        q_expected_2 = self.qnetwork_local_2(states_constraints).gather(1, actions)\n",
    "        ### Loss calculation (we used Mean squared error)\n",
    "        loss_1 = F.mse_loss(q_expected_1, q_targets)\n",
    "        self.optimizer_1.zero_grad()\n",
    "        loss_1.backward()\n",
    "        self.optimizer_1.step()\n",
    "        \n",
    "        loss_2 = F.mse_loss(q_expected_2, q_targets)\n",
    "        self.optimizer_2.zero_grad()\n",
    "        loss_2.backward()\n",
    "        self.optimizer_2.step()\n",
    "\n",
    "        ### Calculate target cost from bellman equation\n",
    "        c_targets_c_1 = self.qnetwork_forward_target_1(next_states_next_constraints).detach().max(1)[0].unsqueeze(1)\n",
    "        c_targets_c_2 = self.qnetwork_forward_target_2(next_states_next_constraints).detach().max(1)[0].unsqueeze(1)\n",
    "        c_targets_c = c_targets_c_1\n",
    "        for i in range(len(c_targets_c_1)):\n",
    "            c_targets_c[i] = max(c_targets_c_1[i],c_targets_c_2[i])\n",
    "        \n",
    "        print()\n",
    "        ### Calculate expected cost from qnetwork_forward\n",
    "        c_expected_c_1 = self.qnetwork_forward_1(states_constraints).gather(1, actions)\n",
    "        ### Loss calculation (we used Mean squared error)\n",
    "        loss_c_1 = F.mse_loss(c_expected_c_1, c_targets_c)\n",
    "        self.optimizer_c_1.zero_grad()\n",
    "        loss_c_1.backward()\n",
    "        self.optimizer_c_1.step()\n",
    "        \n",
    "        c_expected_c_2 = self.qnetwork_forward_2(states_constraints).gather(1, actions)\n",
    "        ### Loss calculation (we used Mean squared error)\n",
    "        loss_c_2 = F.mse_loss(c_expected_c_2, c_targets_c)\n",
    "        self.optimizer_c_2.zero_grad()\n",
    "        loss_c_2.backward()\n",
    "        self.optimizer_c_2.step()\n",
    "        \n",
    "\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local_1, self.qnetwork_target_1, TAU)\n",
    "        self.soft_update(self.qnetwork_local_2, self.qnetwork_target_2, TAU)\n",
    "        self.soft_update(self.qnetwork_forward_1, self.qnetwork_forward_target_1, TAU)\n",
    "        self.soft_update(self.qnetwork_forward_2, self.qnetwork_forward_target_2, TAU)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "compatible-characterization",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T15:51:39.264536Z",
     "start_time": "2023-01-18T15:51:39.234615Z"
    }
   },
   "outputs": [],
   "source": [
    "class SAC_Agent_New():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, constraint,alpha=0.05, gamma=0.99, seed=0, update_every=4, batch_size=128):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.constraint = constraint\n",
    "        self.seed = random.seed(seed)\n",
    "        self.gamma= gamma\n",
    "        self.update_interval = update_every\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Q-Networks\n",
    "        self.qnetwork_local_1 = QNetwork(state_size+1, action_size, seed).to(device)\n",
    "        self.qnetwork_target_1 = QNetwork(state_size+1, action_size, seed).to(device)\n",
    "        self.optimizer_1 = optim.Adam(self.qnetwork_local_1.parameters(), lr=alpha)\n",
    "        \n",
    "        self.qnetwork_local_2 = QNetwork(state_size+1, action_size, seed).to(device)\n",
    "        self.qnetwork_target_2 = QNetwork(state_size+1, action_size, seed).to(device)\n",
    "        self.optimizer_2 = optim.Adam(self.qnetwork_local_2.parameters(), lr=alpha)\n",
    "        \n",
    "        # Q-Cost-Network\n",
    "        self.qnetwork_forward_1 = OneHotDQN(state_size+1, action_size).to(device)\n",
    "        self.qnetwork_forward_target_1 = OneHotDQN(state_size+1, action_size).to(device)\n",
    "        self.optimizer_c_1 = optim.Adam(self.qnetwork_forward_1.parameters(), lr=alpha)\n",
    "        \n",
    "        self.qnetwork_forward_2 = OneHotDQN(state_size+1, action_size).to(device)\n",
    "        self.qnetwork_forward_target_2 = OneHotDQN(state_size+1, action_size).to(device)\n",
    "        self.optimizer_c_2 = optim.Adam(self.qnetwork_forward_2.parameters(), lr=alpha)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = Safe_ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        self.transition_matrix = np.zeros((state_size, action_size, state_size))\n",
    "        self.transition_matrix = np.zeros((state_size, action_size, state_size))\n",
    "        # Occupancy measure\n",
    "        self.occupancy = OccupancyVector()\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, constraint, action, reward, next_state, next_constraint, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state,constraint, action, reward, next_state, next_constraint, done)\n",
    "        self.occupancy.add(state, next_state)\n",
    "        #update state transition matrix\n",
    "        self.transition_matrix[np.argmax(state)][action][np.argmax(next_state)] = 1\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.update_interval\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.gamma)\n",
    "\n",
    "    def get_regret(self, state, prev_state, action):\n",
    "        self.qnetwork_local.eval()\n",
    "        #get regret for current action given potential true states\n",
    "        #potential true states given previous state\n",
    "        true_states = np.argmax(self.transition_matrix[np.argmax(prev_state)], axis=1)\n",
    "        true_states = np.array([statehelper(s) for s in true_states])\n",
    "        actions = np.full((self.action_size,1), action)\n",
    "        #print(state, true_states)\n",
    "        assert state in true_states\n",
    "\n",
    "        #expected value of current s,a\n",
    "        s = torch.tensor(state).float().unsqueeze(0).to(device)\n",
    "        a = torch.tensor([action]).long().unsqueeze(0).to(device)\n",
    "        v = self.qnetwork_local(s).gather(1, a).cpu().detach().numpy()\n",
    "        v = max(v,0)\n",
    "\n",
    "        #expected value of potential observations\n",
    "        ts = torch.tensor(true_states).float().unsqueeze(0).to(device)[0]\n",
    "        ta = torch.tensor(actions).long().unsqueeze(0).to(device)[0]\n",
    "        v_alt = self.qnetwork_local(ts).gather(1, ta).cpu().detach().numpy()\n",
    "        v_alt = np.where(v_alt<0,0,v_alt)\n",
    "\n",
    "        #assert v in v_alt\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        #regret calculation V(s,a) - min_V(true_s,a)\n",
    "        regret = np.max(v_alt) - v\n",
    "        if not isinstance(regret, np.ndarray):\n",
    "            return regret\n",
    "        return regret[0][0]\n",
    "\n",
    "    def get_regret_lower(self, state, prev_state, action):\n",
    "        #regret for lower policy, but meta agent contains actual values\n",
    "        self.qnetwork_local.eval()\n",
    "        #get regret for current action given potential true states\n",
    "        #potential true states given previous state\n",
    "        true_states = np.array([perturb_state(np.argmax(prev_state), a) for a in range(env.action_space.n)])\n",
    "        true_states = np.array([statehelper(s) for s in true_states])\n",
    "        actions = np.array([[perturb_state(np.argmax(s), action)] for s in true_states])\n",
    "        assert state in true_states\n",
    "\n",
    "        #expected value of current s,a\n",
    "        s = torch.tensor(state).float().unsqueeze(0).to(device)\n",
    "        a = torch.tensor([perturb_state(np.argmax(state), action)]).long().unsqueeze(0).to(device)\n",
    "        v = self.qnetwork_local(s).gather(1, a).cpu().detach().numpy()\n",
    "        v = max(v,0)\n",
    "\n",
    "        #expected value of potential observations\n",
    "        ts = torch.tensor(true_states).float().unsqueeze(0).to(device)[0]\n",
    "        ta = torch.tensor(actions).long().unsqueeze(0).to(device)[0]\n",
    "        v_alt = self.qnetwork_local(ts).gather(1, ta).cpu().detach().numpy()\n",
    "        v_alt = np.where(v_alt<0,0,v_alt)\n",
    "\n",
    "        assert s in ts\n",
    "        #assert a in ta\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        #regret calculation V(s,a) - min_V(true_s,a)\n",
    "        regret = np.max(v_alt) - v\n",
    "        if not isinstance(regret, np.ndarray):\n",
    "            return regret\n",
    "        return regret[0][0]\n",
    "\n",
    "    def act(self, state, constraint, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        cons = 0\n",
    "        lamb = 0.5\n",
    "        if state[3].item()>0.25 and 0.5<state[2].item()<1:\n",
    "                cons = 1  \n",
    "        state_constraint = torch.cat((state, constraint))\n",
    "        if(torch.is_tensor(state_constraint)):\n",
    "            state_constraint = state_constraint.float().unsqueeze(0).to(device)\n",
    "        else:\n",
    "            state_constraint = torch.from_numpy(state_constraint).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local_1.eval()\n",
    "        self.qnetwork_local_2.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = torch.zeros(1,self.action_size).to(device)\n",
    "            for i in range(self.action_size):\n",
    "                action_values[0][i] = min(self.qnetwork_local_1(state_constraint)[0][i],self.qnetwork_local_2(state_constraint)[0][i])\n",
    "        self.qnetwork_local_1.train()\n",
    "        self.qnetwork_local_2.train()\n",
    "        \n",
    "        c_f_1 = self.qnetwork_forward_1(state_constraint)\n",
    "        c_f_2 = self.qnetwork_forward_2(state_constraint)\n",
    "        c_f = c_f_1\n",
    "        for i in range(len(c_f)):\n",
    "            c_f[0][i] = max(c_f_1[0][i],c_f_2[0][i])\n",
    "        quantity_1 = c_f + constraint[0].item()\n",
    "        quantity_2 = self.constraint + cons\n",
    "        \n",
    "        constraint_mask = torch.le(quantity_1, quantity_2).float().squeeze(0)\n",
    "        filtered_Q = (action_values + 1000.0) * (constraint_mask)\n",
    "        filtered_action = np.argmax(filtered_Q.cpu().data.numpy())\n",
    "        \n",
    "        alt_Q = action_values - lamb * c_f\n",
    "        alt_action =np.argmax(alt_Q.cpu().data.numpy())\n",
    "        c_sum = constraint_mask.sum(0)\n",
    "        action_mask = (c_sum == torch.zeros_like(c_sum)).cpu().numpy()\n",
    "        action = (1 - action_mask) * filtered_action + action_mask * alt_action\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            #return np.argmax(action_values.cpu().data.numpy())\n",
    "            return action\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        alpha = 0.9\n",
    "        # Obtain random minibatch of tuples from D\n",
    "        states, constraints, actions, rewards, next_states, next_constraints, dones = experiences\n",
    "       \n",
    "        ## Compute and minimize the loss\n",
    "        ### Extract next maximum estimated value from target network\n",
    "        next_states_next_constraints=torch.cat((next_states,next_constraints),dim=1)\n",
    "        states_constraints=torch.cat((states,constraints),dim=1)\n",
    "        \n",
    "        ## Compute and minimize the loss\n",
    "        ### Extract next maximum estimated value from target network\n",
    "        q_targets_next_1 = self.qnetwork_target_1(next_states_next_constraints).detach().max(1)[0].unsqueeze(1)\n",
    "        q_targets_next_2 = self.qnetwork_target_2(next_states_next_constraints).detach().max(1)[0].unsqueeze(1)\n",
    "        q_targets_next = q_targets_next_1\n",
    "        for i in range(len(q_targets_next_1)):\n",
    "            q_targets_next[i] = min(q_targets_next_1[i],q_targets_next_2[i]) - alpha*math.log(((actions == actions[i].item()).sum()/len(actions)).item())\n",
    "        ### Calculate target value from bellman equation\n",
    "        q_targets = rewards + gamma * q_targets_next * (1 - dones)\n",
    "        ### Calculate expected value from local network\n",
    "        q_expected_1 = self.qnetwork_local_1(states_constraints).gather(1, actions)\n",
    "        q_expected_2 = self.qnetwork_local_2(states_constraints).gather(1, actions)\n",
    "        ### Loss calculation (we used Mean squared error)\n",
    "        loss_1 = F.mse_loss(q_expected_1, q_targets)\n",
    "        self.optimizer_1.zero_grad()\n",
    "        loss_1.backward()\n",
    "        self.optimizer_1.step()\n",
    "        \n",
    "        loss_2 = F.mse_loss(q_expected_2, q_targets)\n",
    "        self.optimizer_2.zero_grad()\n",
    "        loss_2.backward()\n",
    "        self.optimizer_2.step()\n",
    "\n",
    "        ### Calculate target cost from bellman equation\n",
    "        c_targets_c_1 = self.qnetwork_forward_target_1(next_states_next_constraints).detach().max(1)[0].unsqueeze(1)\n",
    "        c_targets_c_2 = self.qnetwork_forward_target_2(next_states_next_constraints).detach().max(1)[0].unsqueeze(1)\n",
    "        c_targets_c = c_targets_c_1\n",
    "        for i in range(len(c_targets_c_1)):\n",
    "            c_targets_c[i] = max(c_targets_c_1[i],c_targets_c_2[i])\n",
    "        ### Calculate expected cost from qnetwork_forward\n",
    "        c_expected_c_1 = self.qnetwork_forward_1(states_constraints).gather(1, actions)\n",
    "        ### Loss calculation (we used Mean squared error)\n",
    "        loss_c_1 = F.mse_loss(c_expected_c_1, c_targets_c)\n",
    "        self.optimizer_c_1.zero_grad()\n",
    "        loss_c_1.backward()\n",
    "        self.optimizer_c_1.step()\n",
    "        \n",
    "        c_expected_c_2 = self.qnetwork_forward_2(states_constraints).gather(1, actions)\n",
    "        ### Loss calculation (we used Mean squared error)\n",
    "        loss_c_2 = F.mse_loss(c_expected_c_2, c_targets_c)\n",
    "        self.optimizer_c_2.zero_grad()\n",
    "        loss_c_2.backward()\n",
    "        self.optimizer_c_2.step()\n",
    "        \n",
    "\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local_1, self.qnetwork_target_1, TAU)\n",
    "        self.soft_update(self.qnetwork_local_2, self.qnetwork_target_2, TAU)\n",
    "        self.soft_update(self.qnetwork_forward_1, self.qnetwork_forward_target_1, TAU)\n",
    "        self.soft_update(self.qnetwork_forward_2, self.qnetwork_forward_target_2, TAU)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-flower",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdd61b9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T15:51:39.310428Z",
     "start_time": "2023-01-18T15:51:39.296450Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "class Regret_ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"reward_min\",\"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, reward_min, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, reward_min, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        rewardmins = torch.from_numpy(np.vstack([e.reward_min for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, rewardmins, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "class OccupancyVector:#stores state visitation counts\n",
    "    def __init__(self):\n",
    "        self.state_dict = {}\n",
    "\n",
    "    def add(self, key, val):\n",
    "        sa_tuple = tuple([tuple(np.round(key,2)), tuple(np.round(val,2))])\n",
    "        if sa_tuple not in self.state_dict.keys():\n",
    "            self.state_dict[sa_tuple] = 1\n",
    "\n",
    "    def get_width(self):\n",
    "        return len(self.state_dict.values())\n",
    "\n",
    "    def get_depth(self):\n",
    "        return max(self.state_dict.values())\n",
    "\n",
    "    def get_size(self):\n",
    "        return sum(self.state_dict.values())\n",
    "\n",
    "    def get_occupancy(self, key, val):\n",
    "        sa_tuple = tuple([tuple(np.round(key,2)), tuple(np.round(val,2))])\n",
    "        if sa_tuple not in self.state_dict.keys():\n",
    "            return 0\n",
    "        return self.state_dict[sa_tuple]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "experienced-creation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T15:51:39.326370Z",
     "start_time": "2023-01-18T15:51:39.311409Z"
    }
   },
   "outputs": [],
   "source": [
    "class Safe_ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"constraint\",\"action\", \"reward\", \"next_state\", \"next_constraint\",\"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, constraint, action, reward, next_state, next_constraint,done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, constraint, action, reward, next_state, next_constraint, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        constraints = torch.from_numpy(np.vstack([e.constraint for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        next_constraints = torch.from_numpy(np.vstack([e.next_constraint for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, constraints, actions, rewards, next_states, next_constraints, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-saskatchewan",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d473a1c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T15:51:39.357288Z",
     "start_time": "2023-01-18T15:51:39.343327Z"
    }
   },
   "outputs": [],
   "source": [
    "def dqn(agent, n_episodes=50000, max_t=1000, eps_start=1.0, eps_end=0.001, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    constraints = []\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        constraint = torch.zeros([1])\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            state = torch.from_numpy(state)\n",
    "            if state[3].item()>0.25 and 0<state[2].item()<=0.5:\n",
    "                cons = torch.ones(constraint.shape)\n",
    "            else:\n",
    "                cons = torch.zeros(constraint.shape)\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            s_ = next_state\n",
    "            next_constraint = constraint + cons\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            constraint = next_constraint\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        constraints.append(next_constraint[0].item())\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f} \\tMax Score: {:.2f} \\tScore: {:.2f}'.format(i_episode, np.mean(scores_window), np.max(scores_window), score), end=\"\")\n",
    "        if np.mean(scores_window) >= np.max(scores_window) * 0.8 and i_episode > 400:\n",
    "            pass\n",
    "    return scores,constraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "pointed-skill",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T15:51:39.372262Z",
     "start_time": "2023-01-18T15:51:39.358284Z"
    }
   },
   "outputs": [],
   "source": [
    "def BVF_dqn(BVF_agent, n_episodes=50000, max_t=1000, eps_start=1.0, eps_end=0.001, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    constraints = []\n",
    "    Qcostfor = []\n",
    "    Qcostback = []\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        constraint = torch.zeros([1])\n",
    "        score = 0\n",
    "        ForQ = []\n",
    "        BackQ = []\n",
    "        for t in range(max_t):\n",
    "            state = torch.from_numpy(state)\n",
    "            if state[3].item()>0.25 and 0<state[2].item()<=0.5:\n",
    "                cons = torch.ones(constraint.shape)\n",
    "            else:\n",
    "                cons = torch.zeros(constraint.shape)\n",
    "            action, qfor, qback = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            s_ = next_state\n",
    "            next_constraint = constraint + cons\n",
    "            agent.step(state, constraint ,action, reward, next_state, next_constraint, done)\n",
    "            state = next_state\n",
    "            constraint = next_constraint\n",
    "            score += reward\n",
    "            ForQ.append(qfor)\n",
    "            BackQ.append(qback)\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        constraints.append(next_constraint[0].item())\n",
    "        Qcostfor.append(np.mean(ForQ))\n",
    "        Qcostback.append(np.mean(BackQ))\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f} \\tMax Score: {:.2f} \\tScore: {:.2f}'.format(i_episode, np.mean(scores_window), np.max(scores_window), score), end=\"\")\n",
    "        if np.mean(scores_window) >= np.max(scores_window) * 0.8 and i_episode > 400:\n",
    "            pass\n",
    "    return scores,constraints, Qcostfor, Qcostback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ccdcf51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T15:51:39.388205Z",
     "start_time": "2023-01-18T15:51:39.374242Z"
    }
   },
   "outputs": [],
   "source": [
    "def LYP_dqn(LYP_Agent, n_episodes=50000, max_t=1000, eps_start=1.0, eps_end=0.001, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    constraints = []\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        constraint = torch.zeros([1])\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            state = torch.from_numpy(state)\n",
    "            if state[3].item()>0.25 and 0<state[2].item()<=0.5:\n",
    "                cons = torch.ones(constraint.shape)\n",
    "            else:\n",
    "                cons = torch.zeros(constraint.shape)\n",
    "            action = agent.act(state,eps)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            s_ = next_state\n",
    "            next_constraint = constraint + cons\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            constraint = next_constraint\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        constraints.append(next_constraint[0].item())\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f} \\tMax Score: {:.2f} \\tScore: {:.2f}'.format(i_episode, np.mean(scores_window), np.max(scores_window), score), end=\"\")\n",
    "        if np.mean(scores_window) >= np.max(scores_window) * 0.8 and i_episode > 400:\n",
    "            pass\n",
    "    return scores,constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ahead-magnet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T15:51:39.404162Z",
     "start_time": "2023-01-18T15:51:39.389204Z"
    }
   },
   "outputs": [],
   "source": [
    "def SAC_dqn(SAC_agent, n_episodes=50000, max_t=1000, eps_start=1.0, eps_end=0.001, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    constraints = []\n",
    "    masks = []\n",
    "    Qcost = []\n",
    "    cc = []\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        constraint = torch.zeros([1])\n",
    "        score = 0\n",
    "        mask = []\n",
    "        ForQ = []\n",
    "        cumu_constraints = []\n",
    "        cumu_constraints.append(constraint[0].item())\n",
    "        for t in range(max_t):\n",
    "            state = torch.from_numpy(state)\n",
    "            if state[3].item()>0.25 and 0<state[2].item()<=0.5:\n",
    "                cons = torch.ones(constraint.shape)\n",
    "            else:\n",
    "                cons = torch.zeros(constraint.shape)\n",
    "            action , action_masks, forwardQ = agent.act(state, constraint, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            s_ = next_state\n",
    "            next_constraint = constraint + cons\n",
    "            agent.step(state, constraint ,action, reward, next_state, next_constraint, done)\n",
    "            state = next_state\n",
    "            constraint = next_constraint\n",
    "            score += reward\n",
    "            mask.append(action_masks)\n",
    "            ForQ.append(forwardQ)\n",
    "            cumu_constraints.append(constraint[0].item())\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        constraints.append(next_constraint[0].item())\n",
    "        masks.append(np.mean(mask))\n",
    "        Qcost.append(np.mean(ForQ))\n",
    "        cc.append(np.mean(cumu_constraints))\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f} \\tMax Score: {:.2f} \\tScore: {:.2f}'.format(i_episode, np.mean(scores_window), np.max(scores_window), score), end=\"\")\n",
    "        if np.mean(scores_window) >= np.max(scores_window) * 0.8 and i_episode > 400:\n",
    "            pass\n",
    "    return scores,constraints,masks,Qcost,cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dominican-shareware",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T15:51:39.420120Z",
     "start_time": "2023-01-18T15:51:39.405159Z"
    }
   },
   "outputs": [],
   "source": [
    "def SAC_dqn_New(SAC_agent, n_episodes=50000, max_t=1000, eps_start=1.0, eps_end=0.001, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    constraints = []\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        constraint = torch.zeros([1])\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            state = torch.from_numpy(state)\n",
    "            if state[3].item()>0.25 and 0<state[2].item()<=0.5:\n",
    "                cons = torch.ones(constraint.shape)\n",
    "            else:\n",
    "                cons = torch.zeros(constraint.shape)\n",
    "            action = agent.act(state, constraint, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            s_ = next_state\n",
    "            next_constraint = constraint + cons\n",
    "            agent.step(state, constraint ,action, reward, next_state, next_constraint, done)\n",
    "            state = next_state\n",
    "            constraint = next_constraint\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        constraints.append(next_constraint[0].item())\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f} \\tMax Score: {:.2f} \\tScore: {:.2f}'.format(i_episode, np.mean(scores_window), np.max(scores_window), score), end=\"\")\n",
    "        if np.mean(scores_window) >= np.max(scores_window) * 0.8 and i_episode > 400:\n",
    "            pass\n",
    "    return scores,constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "organized-delight",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T15:51:39.436087Z",
     "start_time": "2023-01-18T15:51:39.421116Z"
    }
   },
   "outputs": [],
   "source": [
    "def Safe_dqn(safe_agent, n_episodes=50000, max_t=1000, eps_start=1.0, eps_end=0.001, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    constraints = []\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        constraint = torch.zeros([1])\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            state = torch.from_numpy(state)\n",
    "            if state[3].item()>0.25 and 0<state[2].item()<=0.5:\n",
    "                cons = torch.ones(constraint.shape)\n",
    "            else:\n",
    "                cons = torch.zeros(constraint.shape)\n",
    "            state_constraint = torch.cat((state, constraint))\n",
    "            action = agent.act(state_constraint, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            s_ = next_state\n",
    "            next_constraint = constraint + cons\n",
    "            if next_constraint[0].item()>agent.constraint:\n",
    "                break\n",
    "            agent.step(state, constraint ,action, reward, next_state, next_constraint, done)\n",
    "            state = next_state\n",
    "            constraint = next_constraint\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        constraints.append(next_constraint[0].item())\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f} \\tMax Score: {:.2f} \\tScore: {:.2f}'.format(i_episode, np.mean(scores_window), np.max(scores_window), score), end=\"\")\n",
    "        if np.mean(scores_window) >= np.max(scores_window) * 0.8 and i_episode > 400:\n",
    "            pass\n",
    "    return scores,constraints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-court",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "486f2413",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T15:51:39.579734Z",
     "start_time": "2023-01-18T15:51:39.516860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (25,)\n",
      "Number of actions:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (5, 5)\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('merge-v0')\n",
    "env = FlattenObsWrapper(env)\n",
    "env.reset(seed=0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)\n",
    "BUFFER_SIZE = int(1e12)  # replay buffer size\n",
    "dqnBATCH_SIZE = 256         # minibatch size\n",
    "dqnGAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "dqnLR = 0.005               # learning rate\n",
    "dqnUPDATE_EVERY = 16        # how often to update the network\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99           # discount factor\n",
    "META_GAMMA=0.99999\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-3              # learning rate\n",
    "META_LR = 5e-5\n",
    "UPDATE_EVERY = 16        # how often to update the network\n",
    "META_UPDATE_EVERY = 32\n",
    "META_BATCH_SIZE = 1024\n",
    "subgoal_timer = 3\n",
    "rLR = 0.05\n",
    "rGAMMA = 0.9\n",
    "rUPDATE_EVERY = 64\n",
    "rBATCH_SIZE = 512\n",
    "adv_ints = [5,4,3,2]\n",
    "adv_int = 1\n",
    "adv_range = 10\n",
    "adv_magnitude = 0.1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e162cf9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T15:06:45.022511Z",
     "start_time": "2023-05-23T15:06:44.619629Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m times \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mreset(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      3\u001b[0m     dqnBATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m         \u001b[38;5;66;03m# minibatch size\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     dqnGAMMA \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.999\u001b[39m            \u001b[38;5;66;03m# discount factor\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "for times in range(5):\n",
    "    env.reset(seed=0)\n",
    "    dqnBATCH_SIZE = 64         # minibatch size\n",
    "    dqnGAMMA = 0.999            # discount factor\n",
    "    TAU = 1e-3              # for soft update of target parameters\n",
    "    dqnLR = 0.005               # learning rate\n",
    "    dqnUPDATE_EVERY = 4        # how often to update the network\n",
    "    BUFFER_SIZE = int(1e16)\n",
    "    dqnBATCH_SIZE = 64         # minibatch size\n",
    "    dqnGAMMA = 0.999            # discount factor\n",
    "    TAU = 1e-3              # for soft update of target parameters\n",
    "    dqnLR = 0.005               # learning rate\n",
    "    dqnUPDATE_EVERY = 4        # how often to update the network\n",
    "    \n",
    "    agent = SAC_Agent_New(state_size=env.observation_space.shape[0], action_size=env.action_space.n, constraint=8, alpha=dqnLR, gamma=dqnGAMMA, seed=0, update_every=dqnUPDATE_EVERY, batch_size=dqnBATCH_SIZE)\n",
    "    sacnewscores,sacnewconstraints = SAC_dqn_New(agent, n_episodes=25000, eps_decay=0.995)\n",
    "    print(np.mean(sacnewscores))\n",
    "    torch.save(sacnewscores,\"results/sacnew/mr\"+str(times))\n",
    "    torch.save(sacnewconstraints,\"results/sacnew/mc\"+str(times))\n",
    "    \n",
    "    agent = BVF_Agent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, constraint=8, alpha=dqnLR, gamma=dqnGAMMA, seed=0, update_every=dqnUPDATE_EVERY, batch_size=dqnBATCH_SIZE)\n",
    "    bvfscores,bvfconstraints, bvfforward, bvfbackward = BVF_dqn(agent, n_episodes=25000, eps_decay=0.995)\n",
    "    print(np.mean(bvfscores))\n",
    "    torch.save(bvfscores,\"results/bvf/mr\"+str(times))\n",
    "    torch.save(bvfconstraints,\"results/bvf/mc\"+str(times))\n",
    "    \n",
    "    agent = Safe_Agent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, constraint=8, alpha=dqnLR, gamma=dqnGAMMA, seed=0, update_every=dqnUPDATE_EVERY, batch_size=dqnBATCH_SIZE)\n",
    "    safescores,safeconstraints = Safe_dqn(agent, n_episodes=25000, eps_decay=0.995)\n",
    "    print(np.mean(safescores))\n",
    "    torch.save(safescores,\"results/safedqn/mr\"+str(times))\n",
    "    torch.save(safeconstraints,\"results/safedqn/mc\"+str(times))\n",
    "    \n",
    "    agent = Agent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, alpha=dqnLR, gamma=dqnGAMMA, seed=0, update_every=dqnUPDATE_EVERY, batch_size=dqnBATCH_SIZE)\n",
    "    scores,constraints = dqn(agent, n_episodes=25000, eps_decay=0.995)\n",
    "    print(np.mean(scores))\n",
    "    torch.save(scores,\"results/normal/mr\"+str(times))\n",
    "    torch.save(constraints,\"results/normal/mc\"+str(times))\n",
    "    \n",
    "    agent = LYP_Agent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, constraint=8, alpha=dqnLR, gamma=dqnGAMMA, seed=0, update_every=dqnUPDATE_EVERY, batch_size=dqnBATCH_SIZE)\n",
    "    lypscores,lypconstraints = LYP_dqn(agent, n_episodes=25000, eps_decay=0.995)\n",
    "    print(np.mean(lypscores))\n",
    "    torch.save(lypscores,\"results/lyp/mr\"+str(times))\n",
    "    torch.save(lypconstraints,\"results/lyp/mc\"+str(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2b6d0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T15:06:45.026506Z",
     "start_time": "2023-05-23T15:06:44.798Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import matplotlib.pyplot as plt\n",
    "window = 300\n",
    "avg_scores = []\n",
    "avg_safescores = []\n",
    "avg_bvfscores = []\n",
    "avg_sacnewscores = []\n",
    "avg_lypscores = []\n",
    "for i in range(5):\n",
    "    scores = torch.load(\"results/normal/mr\"+str(i))\n",
    "    safescores = torch.load(\"results/safedqn/mr\"+str(i))\n",
    "    bvfscores = torch.load(\"results/bvf/mr\"+str(i))\n",
    "    sacnewscores = torch.load(\"results/sacnew/mr\"+str(i))\n",
    "    lypscores = torch.load(\"results/lyp/mr\"+str(i))\n",
    "    scoresdisplay = [np.mean(scores[i-window:i]) for i in range(window,len(scores))]\n",
    "    safescoresdisplay = [np.mean(safescores[i-window:i]) for i in range(window,len(safescores))]\n",
    "    bvfscoresdisplay = [np.mean(bvfscores[i-window:i]) for i in range(window,len(bvfscores))]\n",
    "    sacnewscoresdisplay = [np.mean(sacnewscores[i-window:i]) for i in range(window,len(sacnewscores))]\n",
    "    lypscoresdisplay = [np.mean(lypscores[i-window:i]) for i in range(window,len(lypscores))]\n",
    "    \n",
    "    avg_scores.append(scoresdisplay)\n",
    "    avg_safescores.append(safescoresdisplay)\n",
    "    avg_bvfscores.append(bvfscoresdisplay)\n",
    "    avg_sacnewscores.append(sacnewscoresdisplay)\n",
    "    avg_lypscores.append(lypscoresdisplay)\n",
    "    \n",
    "avg_scores = np.array(avg_scores)\n",
    "avg_safescores = np.array(avg_safescores)\n",
    "avg_bvfscores = np.array(avg_bvfscores)\n",
    "avg_sacnewscores = np.array(avg_sacnewscores)\n",
    "avg_lypscores = np.array(avg_lypscores)\n",
    "\n",
    "mean_scores = np.mean(avg_scores,axis=0)\n",
    "mean_safescores = np.mean(avg_safescores,axis=0)\n",
    "mean_bvfscores = np.mean(avg_bvfscores,axis=0)\n",
    "mean_sacnewscores = np.mean(avg_sacnewscores,axis=0)\n",
    "mean_lypscores = np.mean(avg_lypscores,axis=0)\n",
    "\n",
    "std_scores = np.std(avg_scores,axis=0)\n",
    "std_safescores = np.std(avg_safescores,axis=0)\n",
    "std_bvfscores = np.std(avg_bvfscores,axis=0)\n",
    "std_sacnewscores = np.std(avg_sacnewscores,axis=0)\n",
    "std_lypscores = np.std(avg_lypscores,axis=0)\n",
    "\n",
    "plt.plot(range(0,len(mean_scores)), mean_scores, label=\"Unsafe DQN\")\n",
    "plt.fill_between(range(0,len(mean_scores)), mean_scores + std_scores, mean_scores - std_scores, alpha = 0.1)\n",
    "plt.plot(range(0,len(mean_safescores)), mean_safescores, label=\"Safe DQN\")\n",
    "plt.fill_between(range(0,len(mean_safescores)), mean_safescores + std_safescores, mean_safescores - std_safescores, alpha = 0.1)\n",
    "plt.plot(range(0,len(mean_bvfscores)), mean_bvfscores, label=\"BVF\")\n",
    "plt.fill_between(range(0,len(mean_bvfscores)), mean_bvfscores + std_bvfscores, mean_bvfscores - std_bvfscores, alpha = 0.1)\n",
    "plt.plot(range(0,len(mean_sacnewscores)), mean_sacnewscores, label=\"Safe SAC\")\n",
    "plt.fill_between(range(0,len(mean_sacnewscores)), mean_sacnewscores + std_sacnewscores, mean_sacnewscores - std_sacnewscores, alpha = 0.1)\n",
    "plt.plot(range(0,len(mean_lypscores)), mean_lypscores, label=\"Lyapunov\")\n",
    "plt.fill_between(range(0,len(mean_lypscores)), mean_lypscores + std_lypscores, mean_lypscores - std_lypscores, alpha = 0.1)\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Average Score in each Episode')\n",
    "plt.savefig('Figures/Merge_Scores',dpi=1080)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9650517a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T15:06:45.027499Z",
     "start_time": "2023-05-23T15:06:44.982Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import matplotlib.pyplot as plt\n",
    "window = 300\n",
    "avg_constraints = []\n",
    "avg_safeconstraints = []\n",
    "avg_bvfconstraints = []\n",
    "avg_sacnewconstraints = []\n",
    "avg_lypconstraints = []\n",
    "for i in range(5):\n",
    "    constraints = torch.load(\"results/normal/mc\"+str(i))\n",
    "    safeconstraints = torch.load(\"results/safedqn/mc\"+str(i))\n",
    "    bvfconstraints = torch.load(\"results/bvf/mc\"+str(i))\n",
    "    sacnewconstraints = torch.load(\"results/sacnew/mc\"+str(i))\n",
    "    lypconstraints = torch.load(\"results/lyp/mc\"+str(i))\n",
    "\n",
    "    constraintsdisplay = [np.mean(constraints[i-window:i]) for i in range(window,len(constraints))]\n",
    "    safeconstraintsdisplay = [np.mean(safeconstraints[i-window:i]) for i in range(window,len(safeconstraints))]\n",
    "    bvfconstraintsdisplay = [np.mean(bvfconstraints[i-window:i]) for i in range(window,len(bvfconstraints))]\n",
    "    sacnewconstraintsdisplay = [np.mean(sacnewconstraints[i-window:i]) for i in range(window,len(sacnewconstraints))]\n",
    "    lypconstraintsdisplay = [np.mean(lypconstraints[i-window:i]) for i in range(window,len(lypconstraints))]\n",
    "    \n",
    "    avg_constraints.append(constraintsdisplay)\n",
    "    avg_safeconstraints.append(safeconstraintsdisplay)\n",
    "    avg_bvfconstraints.append(bvfconstraintsdisplay)\n",
    "    avg_sacnewconstraints.append(sacnewconstraintsdisplay)\n",
    "    avg_lypconstraints.append(lypconstraintsdisplay)\n",
    "    \n",
    "avg_constraints = np.array(avg_constraints)\n",
    "avg_safeconstraints = np.array(avg_safeconstraints)\n",
    "avg_bvfconstraints = np.array(avg_bvfconstraints)\n",
    "avg_sacnewconstraints = np.array(avg_sacnewconstraints)\n",
    "avg_lypconstraints = np.array(avg_lypconstraints)\n",
    "\n",
    "mean_constraints = np.mean(avg_constraints,axis=0)\n",
    "mean_safeconstraints = np.mean(avg_safeconstraints,axis=0)\n",
    "mean_bvfconstraints = np.mean(avg_bvfconstraints,axis=0)\n",
    "mean_sacnewconstraints = np.mean(avg_sacnewconstraints,axis=0)\n",
    "mean_lypconstraints = np.mean(avg_lypconstraints,axis=0)\n",
    "\n",
    "std_constraints = np.std(avg_constraints,axis=0)\n",
    "std_safeconstraints = np.std(avg_safeconstraints,axis=0)\n",
    "std_bvfconstraints = np.std(avg_bvfconstraints,axis=0)\n",
    "std_sacnewconstraints = np.std(avg_sacnewconstraints,axis=0)\n",
    "std_lypconstraints = np.std(avg_lypconstraints,axis=0)\n",
    "\n",
    "plt.plot(range(0,len(mean_constraints)), mean_constraints, label=\"Unsafe DQN\")\n",
    "plt.fill_between(range(0,len(mean_constraints)), mean_constraints + std_constraints, mean_constraints - std_constraints, alpha = 0.1)\n",
    "plt.plot(range(0,len(mean_safeconstraints)), mean_safeconstraints, label=\"Safe DQN\")\n",
    "plt.fill_between(range(0,len(mean_safeconstraints)), mean_safeconstraints + std_safeconstraints, mean_safeconstraints - std_safeconstraints, alpha = 0.1)\n",
    "plt.plot(range(0,len(mean_bvfconstraints)), mean_bvfconstraints, label=\"BVF\")\n",
    "plt.fill_between(range(0,len(mean_bvfconstraints)), mean_bvfconstraints + std_bvfconstraints, mean_bvfconstraints - std_bvfconstraints, alpha = 0.1)\n",
    "plt.plot(range(0,len(mean_sacnewconstraints)), mean_sacnewconstraints, label=\"Safe SAC\")\n",
    "plt.fill_between(range(0,len(mean_sacnewconstraints)), mean_sacnewconstraints + std_sacnewconstraints, mean_sacnewconstraints - std_sacnewconstraints, alpha = 0.1)\n",
    "plt.plot(range(0,len(mean_lypconstraints)), mean_lypconstraints, label=\"Lyapunov\")\n",
    "plt.fill_between(range(0,len(mean_lypconstraints)), mean_lypconstraints + std_lypconstraints, mean_lypconstraints - std_lypconstraints, alpha = 0.1)\n",
    "plt.axhline(y=8, color='black', linestyle='--')\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Constraint')\n",
    "plt.title('Average Constraint in each Episode')\n",
    "plt.savefig('Figures/Merge_Constraint',dpi=1080)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fbccdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
